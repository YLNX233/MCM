# 反正就是鬼扯，哎对对对对对

## 深度学习

### 深度学习的building blocks

之前讲了**感知机**。一个感知机可以完成一个线性二分类。对多种特征分层分类，就得到了神经网络。

神经网络的运作就是一层层的感知机对输入的特征进行抽象，我们可以指望网络从一个高维的空间中抽象出了一种不那么显然（不易用编程显式实现的）的分类方法。

### 深度学习能干点啥

神经网络可以放在很多地方。

下面的例子基本都是与深度学习有交集的AI领域

| 。。。。。。|。。。。。。。 | |
|---|---|---|
|分类|**监督学习**|我们有很多的数据，这些数据都有标签（有**先验知识**）。神经网络可以用这些数据训练，训练后的网络多少学得了先验知识，可以对分类未见过的事物。|
|寻找样本特征|**无监督学习**|我们有数据但无足够**先验知识**。因此希望机器自行找到好的特征并据此将之分类。神经网络加持的无监督学习可以很好地在没有先验知识的情况下得到“很合理的分类特征”。本质上人也是一种高性能的无监督学习的模型。|
|最优化|**强化学习**|强化学习是让智能体与环境交互并从中学到策略，以最大化其奖赏。其运行的本质是一种在**马尔科夫过程**上进行的**动态规划**，训练的本质是想办法找马尔科夫过程上奖赏的概率分布。阿尔法go的算法就属于强化学习。对于求解一个最优化序列地的问题，神经网络加持的强化学习可以在麻烦的最优化问题上堆算力来出效果|
|生成|**生成式对抗网络**|包含一个**生成模型**和一个**判别模型**。生成模型根据随机噪音生成东西，判别模型判断东西是否是生成的。生成模型与判别模型构成了一个博弈关系，训练后的生成模型可以用来生成想要的东西，并且以假乱真|

### 训练与调参

深度神经网络中含有很多**感知机**（也叫**神经元**）。因此它有很多参数。这些参数的好坏直接影响模型的效果。可以手动调参，但大多数情况下采用**数据驱动**的方式。

神经网络参数采用**数据驱动**方式的调整被形象地称为“**训练**”。在训练时，定义一个衡量在给定训练数据下模型“好坏”的**损失函数**。对损失函数求关于神经网络参数的梯度并将使用一些**优化算法**将神经网络参数向着**梯度下降**的方向逐步调整，即可以让网络更“好”。

求损失函数关于神经网络梯度的方式叫做**反向传递**，实际上是求梯度的链式法则。由于神经网络可以视作一种计算图，求梯度的链式法则可以很自然地用在这里。

训练这个过程引出了许多新概念。

一些**超参数**：

超参数不是训练过程中模型学得的参数，需要通常需要手动调整或用一些自动调参的办法。

| | |
|-|-|
|learning rate|学习速率。定义了梯度下降的步长|
|epoch|迭代次数。定义了神经网络在训练集中的训练次数|

一些**优化算法**（或者叫“演化策略”）：

这些优化算法负责根据损失函数的梯度以及一些别的玩意求出优化神经网络参数的方向。五花八门，各有千秋（所以细节忘了都）

不过放心吧，我基本只用Adam

| | |
|-|-|
|动量优化|加入动量可以减少梯度下降方向的震荡，加快收敛。注意，这里只是给下一步的计算结果加上了动量，下一步只基于梯度|
|牛顿法(NAG)|动量法的改进。与动量法不同的是在计算下一步方向是考虑了动量。因此下一步的方向不仅与梯度有关，也与当前动量有关|
|Adagrad|自动放缩参数。对于稀疏的特征得到较大幅度的更新，对于稠密的特征采取较小负的更新。用在分类器上可以避免训练时只关注多数而忽略了几个别致的例子|
|RMSProp|在前者的基础上摆脱了对学习速率超参数的依赖|
|Adam|（Adaptive Moment Estimation）自动缩放参数的方法类似动量在阻力下衰减|

模型的**演化策略**不只有梯度下降系列。

一些玄学：

训练程度不当有**欠拟合**、**过拟合**

**欠拟合**需要增加训练次数、增加网络学习能力

**奥卡姆剃刀**的思想告诉我们也要试图防止**过拟合**

| | |
|-|-|
|weight decay|也叫L2正则化。用于防止过拟合。尽量使神经网络中的参数取较小的数|
|learning rate decay|学习速率的下降率。学习速率随时间下降有利于快速（前期）且精确（后期）地找到解|
|Dropout|一种正则化方法，防止过拟合。在每次训练（梯度下降）的过程中随机不激活该神经元。解释一言难尽，姑且玄学|

估计这么多够用了。

## 图论算法

并不试图教会，只打算意会

### 图是什么

G = <V,E>
图是由节点集合和边集合构成的有序二元组。

其中边集合是有序偶<u,v>的称为**有向图**，是无序偶(u,v)的称为**无向图**

以下就是一个无向图

![avatar](/mdpic/GraphSample.png)

### 图的表示、存储

**邻接矩阵**：

$m_{i,j}$表示是否存在边<i,j>

无向图的邻接矩阵是对称阵

**关联矩阵**：

$m_{i,j}$如此定义：

= 1 当$v_i$是有向边$a_j$的始点
= -1 ~终点
= 0 其他

例：

![avator](/mdpic/GraphMatrixSample.png)

其邻接矩阵为：

![avator](/mdpic/LinJieJuZhen.png)

其关联矩阵为：

![avator](/mdpic/GuanLianJuZhen.png)

**邻接表**：

计算机中用矩阵存储稀疏图是浪费的。没几个边，矩阵的大部分都是0

因此往往采用邻接表存储图，邻接表是数据结构中表的一种

邻接表包含一系列头结点，头结点存储一个链子的地址。链子上是从这个头结点出发的所有边

![avator](/mdpic/LingJieBiao.jfif)

### 图的一些性质

**连通性**：

就是字面意思。

对于无向图，只有联通与不联通之分

对于有向图，联通依据强度不同分为三种

1、**强联通**：从任意顶点出发有抵达任意顶点的通路。非退化的强联通有向图必然包含一个含所有节点的环

2、**单向联通**：从任意顶点a与任意顶点b，若没有a->b的通路就必然有b->a的通路。强连通图是单向联通的

3、**弱联通**：直观来说，化为无向图后是联通的。

由连通性引出的一些算法

判断连通性：

这个很随意，跑一遍多源最短路或者搜索的同时维护并查集都行。值得注意的是单源的算法如单源最短路和Prim只能判断从当前点出发到其他节点的连通性，不等价于整个图的连通性。

**顶点基**：

在不强联通的图中的一个顶点集合，若从其中某个节点出发可以抵达其余所有节点，而其子集不能，则这是一个顶点基。顶点基不唯一，但其中含有节点数目相同

![avator](/mdpic/DingDianJi.png)

上图中的顶点基有{u,t}、{v,t}、{w,t}

**强连通分量**：

强连通分量全称**强联通分子图**。非常直白，就是一个子图，强联通且最大就叫强连通分量

由强连通分量引出的算法：

求强连通分量：我们并没有学。正反两遍bfs即可

压缩图：因为强联联通分子图内部的节点可以抵达彼此，在求一些与连通性相关的问题时，可以考虑将强连通分量压缩成一个节点，保留强连通分量到外部的所有连接。

